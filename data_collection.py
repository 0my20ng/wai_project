import pandas as pd
import requests
from bs4 import BeautifulSoup
import datetime
import xml.etree.ElementTree as ET

# ==========================================
# 1. í™˜ê²½ ì„¤ì • (Configuration)
# ==========================================
# ë‹¤ìš´ë¡œë“œ ë°›ì€ CSV íŒŒì¼ëª…
CSV_FILE_PATH = r'C:\Users\11\Desktop\wai_project\í•œêµ­ì‚¬íšŒë³´ì¥ì •ë³´ì›_ë¯¼ê°„ë³µì§€ì„œë¹„ìŠ¤ì •ë³´_20251105.csv'

# ê³µê³µë°ì´í„°í¬í„¸ì—ì„œ ë°œê¸‰ë°›ì€ ì²­ë…„ì •ì±… API í‚¤ (ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”)
API_KEY = "30665de9-6085-43b3-980a-f9e94d4fe2f0" 

# ==========================================
# 2. ì •í˜• ë°ì´í„° ìˆ˜ì§‘: CSV íŒŒì¼ ë¡œë“œ
# ==========================================
def load_csv_data(file_path):
    print(f"ğŸ”„ [CSV] '{file_path}' ë¡œë”© ì¤‘...")
    try:
        # í•œê¸€ ì¸ì½”ë”© ì²˜ë¦¬ (cp949 ë˜ëŠ” utf-8-sig)
        df = pd.read_csv(file_path, encoding='cp949')
    except UnicodeDecodeError:
        df = pd.read_csv(file_path, encoding='utf-8')
    except FileNotFoundError:
        print("âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return pd.DataFrame()

    # í•„ìš”í•œ í•µì‹¬ ì»¬ëŸ¼ë§Œ ì„ íƒ
    # ì‹¤ì œ ì»¬ëŸ¼ëª…: 'ì‚¬ì—…ëª…', 'ì§€ì›ëŒ€ìƒ', 'ì§€ì›ë‚´ìš©', 'ì‹ ì²­ë°©ë²•', 'ì‚¬ì—…ì¢…ë£Œì¼'
    selected_cols = ['ì‚¬ì—…ëª…', 'ì§€ì›ëŒ€ìƒ', 'ì§€ì›ë‚´ìš©', 'ì‹ ì²­ë°©ë²•', 'ì‚¬ì—…ì¢…ë£Œì¼']
    df = df[selected_cols]
    
    # ì¢…ë£Œëœ ì‚¬ì—… í•„í„°ë§ (ì˜ˆ: 2025ë…„ ì´ì „ ì¢…ë£Œ ì‚¬ì—… ì œì™¸)
    # ë‚ ì§œ í˜•ì‹ì´ ì œê°ê°ì¼ ìˆ˜ ìˆì–´ ë¬¸ìì—´ ë¹„êµë¡œ ê°„ë‹¨íˆ ì²˜ë¦¬í•˜ê±°ë‚˜ ìƒëµ ê°€ëŠ¥
    df['ì¶œì²˜'] = 'ë¯¼ê°„ë³µì§€(CSV)'
    
    print(f"âœ… [CSV] ë¡œë“œ ì™„ë£Œ: {len(df)}ê±´")
    return df

# ==========================================
# 3. ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘: Open API í˜¸ì¶œ
# ==========================================
def fetch_api_data(api_key):
    print("ğŸ”„ [API] ì‹¤ì œ ì²­ë…„ì •ì±… ë°ì´í„° ìš”ì²­ ì¤‘...")
    
    print("ğŸ”„ [API] ì‹¤ì œ ì²­ë…„ì •ì±… ë°ì´í„° ìš”ì²­ ì¤‘...")
    
    url = "https://www.youthcenter.go.kr/go/ythip/getPlcy"
    
    # ê¸°ë³¸ íŒŒë¼ë¯¸í„°
    params = {
        'apiKeyNm': api_key,
        'pageSize': 100,         # í•œ í˜ì´ì§€ë‹¹ 100ê°œ
        'rtnType': 'json'
    }
    
    all_policies = []
    page = 1
    total_count = 0
    
    while True:
        params['pageNum'] = page
        print(f"   PLEASE WAIT... í˜ì´ì§€ {page} ìš”ì²­ ì¤‘...")
        
        try:
            # 3. ì‹¤ì œ ìš”ì²­ ë³´ë‚´ê¸°
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
            }
            
            response = requests.get(url, params=params, headers=headers, verify=False)
            
            if response.status_code != 200:
                print(f"âŒ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                break
                
            data = response.json()
            
            # ì´ ê°œìˆ˜ í™•ì¸ (ì²« í˜ì´ì§€ì—ì„œë§Œ)
            if page == 1:
                # êµ¬ì¡°: data['result']['pagging']['totCount']
                try:
                    total_count = data['result']['pagging']['totCount']
                    print(f"ğŸ“Š ì´ ë°ì´í„° ê°œìˆ˜: {total_count}ê°œ ë°œê²¬")
                except:
                    pass

            items = []
            # ë°ì´í„° ì¶”ì¶œ ë¡œì§
            if 'youthPolicyList' in data:
                items = data['youthPolicyList']
            elif 'result' in data and isinstance(data['result'], dict):
                if 'youthPolicyList' in data['result']:
                    items = data['result']['youthPolicyList']
            
            if not items:
                print("   ï¿½ ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
                
            for item in items:
                if not isinstance(item, dict): continue

                min_age = item.get('sprtTrgtMinAge', '')
                max_age = item.get('sprtTrgtMaxAge', '')
                age_info = f"ë§Œ {min_age}ì„¸ ~ {max_age}ì„¸" if min_age and max_age else item.get('ageInfo', '')

                policy = {
                    'ì‚¬ì—…ëª…': item.get('plcyNm', item.get('polyBizSjnm', '')), 
                    'ì§€ì›ëŒ€ìƒ': age_info, 
                    'ì§€ì›ë‚´ìš©': item.get('plcySprtCn', item.get('polyItcnCn', '')),
                    'ì‹ ì²­ë°©ë²•': item.get('plcyAplyMthdCn', item.get('rqutProcCn', '')),
                    'ìƒì„¸ë§í¬': item.get('aplyUrlAddr', item.get('rqutUrla', ''))
                }
                all_policies.append(policy)
            
            # ì¢…ë£Œ ì¡°ê±´ í™•ì¸
            if len(all_policies) >= total_count and total_count > 0:
                print("âœ… ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
                break
                
            page += 1
            
        except Exception as e:
            print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            break

    df_api = pd.DataFrame(all_policies)
    df_api['ì¶œì²˜'] = 'ì²­ë…„ì •ì±…(API)'
    print(f"âœ… [API] ìµœì¢… ìˆ˜ì§‘ ì™„ë£Œ: {len(df_api)}ê±´")
    return df_api

    # ========================================================
    # ğŸ‘‡ Mock DataëŠ” ì´ì œ ì£¼ì„ ì²˜ë¦¬ (ì‹¤ì œ í‚¤ê°€ ì—†ì„ ë•Œë§Œ ì‚¬ìš©)
    # ========================================================
    # mock_data = [
    #     {
    #         'ì‚¬ì—…ëª…': '[í…ŒìŠ¤íŠ¸] ì²­ë…„ì›”ì„¸ì§€ì›',
    #         'ì§€ì›ëŒ€ìƒ': 'ë§Œ 19ì„¸~34ì„¸',
    #         'ì§€ì›ë‚´ìš©': 'ì›” 20ë§Œì›',
    #         'ì‹ ì²­ë°©ë²•': 'ë³µì§€ë¡œ',
    #         'ì‚¬ì—…ì¢…ë£Œì¼': '2026-12-31'
    #     }
    # ]
    # return pd.DataFrame(mock_data)

# ==========================================
# 4. ìƒì„¸ ì •ë³´ ìˆ˜ì§‘: ì›¹ í¬ë¡¤ë§ (Web Crawling)
# ==========================================
def crawl_detail_content(url):
    """
    ê³µê³  ìƒì„¸ í˜ì´ì§€ URLì„ ë°›ì•„ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê¸ì–´ì˜¤ëŠ” í•¨ìˆ˜
    """
    headers = {'User-Agent': 'Mozilla/5.0'}
    try:
        response = requests.get(url, headers=headers, timeout=5)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ì¶° íƒœê·¸ ìˆ˜ì • í•„ìš” (ì˜ˆ: div.view_cont)
            # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ body í…ìŠ¤íŠ¸ ì „ì²´ë¥¼ ê°€ì ¸ì˜´
            text = soup.body.get_text(strip=True)
            return text[:500] + "..." # ë„ˆë¬´ ê¸°ë‹ˆê¹Œ ì•ë¶€ë¶„ë§Œ ìë¦„
        return ""
    except:
        return "í¬ë¡¤ë§ ì‹¤íŒ¨"

# ==========================================
# 5. ë©”ì¸ ì‹¤í–‰ (Main Execution)
# ==========================================
if __name__ == "__main__":
    # 1. CSV ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    df_csv = load_csv_data(CSV_FILE_PATH)
    
    # 2. API ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    df_api = fetch_api_data(API_KEY)
    
    # 3. ë°ì´í„° í†µí•© (Merge)
    final_df = pd.concat([df_csv, df_api], ignore_index=True)
    
    # 4. ê²°ì¸¡ì¹˜ ê°„ë‹¨ ì²˜ë¦¬
    final_df = final_df.fillna("ë‚´ìš© ì—†ìŒ")
    
    # 5. ê²°ê³¼ í™•ì¸
    print("\n" + "="*40)
    print("ğŸš€ [Step 1] í†µí•© ë°ì´í„° êµ¬ì¶• ì™„ë£Œ")
    print("="*40)
    print(f"ì´ ë°ì´í„° ê°œìˆ˜: {len(final_df)}ê°œ")
    print(final_df.head())
    
    # 6. íŒŒì¼ë¡œ ì €ì¥ (ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•´)
    final_df.to_csv("total_welfare_data.csv", index=False, encoding='utf-8-sig')
    print("\nğŸ’¾ 'total_welfare_data.csv' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")